{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projectfinal.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMlai5IbIE/ZG43EK422DYJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinuElsa/project/blob/main/projectfinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Liberaries"
      ],
      "metadata": {
        "id": "3WFWQh4GkFYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion \n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import SGDClassifier, RidgeClassifier\n",
        "from sklearn.feature_extraction import text \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "hOmdth2_kO7I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "849f76f6-52e3-4939-ec56-6562bda97cde"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-263e2fba8213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInteger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skopt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the three csv documents\n",
        "comments = pd.read_csv('attack_annotated_comments.csv', sep = '\\t', index_col = 0)\n",
        "annotations = pd.read_csv('attack_annotations.csv',  sep = '\\t')\n",
        "demographics = pd.read_csv('attack_worker_demographics.csv', sep = '\\t')"
      ],
      "metadata": {
        "id": "8ZEopw7wq12C"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the demographic data to the annotations data, then blow it\n",
        "# out into boolean series (technically 1s and 0s) and drop any irrelevant or pre-blown-out columns\n",
        "annotWithDemo = pd.merge(annotations, demographics, on='worker_id', how='left')\n",
        "annotWithDemo.drop(columns=['quoting_attack','recipient_attack','third_party_attack',\n",
        "                            'other_attack', 'worker_id'], inplace=True)\n",
        "boolCols = annotWithDemo.join(annotWithDemo.gender.str.get_dummies())\n",
        "boolCols.drop(columns='gender',inplace=True)\n",
        "boolCols = boolCols.join(boolCols.age_group.str.get_dummies())\n",
        "boolCols.drop(columns='age_group',inplace=True)\n",
        "boolCols = boolCols.join(boolCols.education.str.get_dummies())\n",
        "boolCols.drop(columns='education',inplace=True)\n",
        "boolCols['no_degree'] = boolCols['none'] + boolCols['some'] + boolCols['hs']\n",
        "boolCols['college_degree'] = boolCols['bachelors'] + boolCols['doctorate'] + boolCols['masters'] + boolCols['professional']\n",
        "boolCols.drop(columns=['bachelors','doctorate','hs','masters','none','professional','some'],\n",
        "              inplace=True)"
      ],
      "metadata": {
        "id": "8xthQNXzroZX",
        "outputId": "3583d221-7523-49da-fdab-d3c38f05ef50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4bb36701b3e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Join the demographic data to the annotations data, then blow it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# out into boolean series (technically 1s and 0s) and drop any irrelevant or pre-blown-out columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mannotWithDemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemographics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'worker_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m annotWithDemo.drop(columns=['quoting_attack','recipient_attack','third_party_attack',\n\u001b[1;32m      5\u001b[0m                             'other_attack', 'worker_id'], inplace=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     )\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_rkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1777\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1779\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'worker_id'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a data frame containing only reviews with at least one attack identified\n",
        "# Group both data frames by rev_id, the \"attack only\" frame will be used as the numerator in\n",
        "# finding pctg of each demographic column that labeled a review an attack\n",
        "boolColsAttackOnly = boolCols.loc[boolCols['attack'] > 0]\n",
        "boolColsAttackOnlyGrouped = boolColsAttackOnly.groupby('rev_id', as_index=False).sum()\n",
        "boolColsGrouped = boolCols.groupby('rev_id', as_index=False).sum()"
      ],
      "metadata": {
        "id": "XI8jV3gjrwj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the demographic columns into percentages and find the overall pct of\n",
        "# annotators marking a comment as an attack to aid in classifying comments\n",
        "allRev= boolColsGrouped['rev_id'].to_frame(\"rev_id\")\n",
        "allRevAttackOnlyGrouped = pd.merge(allRev, boolColsAttackOnlyGrouped, on='rev_id', how='left')\n",
        "demo = allRevAttackOnlyGrouped.loc[:,'english_first_language':].div(boolColsGrouped.loc[:,'english_first_language':])\n",
        "totalAnnotators = boolCols.groupby('rev_id', as_index=False).count()['attack']\n",
        "attack = boolColsGrouped['attack'].div(totalAnnotators).to_frame('pctAttack')"
      ],
      "metadata": {
        "id": "LE-a_Su9sAWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the max demographic percentage that advocated for attack in each row and add it to the attack\n",
        "# dataframe. Create an attack column for the target labels and flip any rows meeting the criteria\n",
        "# to True. Insert the rev_id column into the attack frame \n",
        "demoMax = demo.loc[:,'english_first_language':].max(axis = 1)\n",
        "attack.insert(1,'demoMax',demoMax)\n",
        "attack['attack'] = False\n",
        "attack.loc[(attack['pctAttack'] >= .5) | (attack['demoMax'] > .5), 'attack'] = True\n",
        "attack.loc[attack['pctAttack'] <.25,'attack'] = False\n",
        "attack.insert(0,'rev_id',boolColsGrouped['rev_id'])\n",
        "labels = attack.drop(columns=['demoMax', 'pctAttack'])"
      ],
      "metadata": {
        "id": "l4W3U0qvsF7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the labels data frame by dropping irrelevant columns from the attack frame and merge the\n",
        "# labels into the comments dataframe to complete labeling all comments\n",
        "labels = attack.drop(columns=['demoMax', 'pctAttack'])\n",
        "comments = pd.merge(comments, labels, on='rev_id', how='left')"
      ],
      "metadata": {
        "id": "nfrfQ1kTsLtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Cleaning the data"
      ],
      "metadata": {
        "id": "8vUjkwfcsYlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove newline and tab tokens\n",
        "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
        "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
      ],
      "metadata": {
        "id": "5cOwlSGmsZyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split and encode the training data\n",
        "X_train,X_test,y_train,y_test = train_test_split(\n",
        "    comments.comment, comments.attack, test_size=.33,random_state=42)\n",
        "\n",
        "encode = LabelEncoder()\n",
        "y_train = encode.fit_transform(y_train)\n",
        "y_test = encode.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "iai-cWRMsgYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the parameter grid, I kept it separate for ease in tweaking values:\n",
        "\n",
        "parameterGrid = dict(\n",
        "    features__word__max_features=[10000],\n",
        "    features__word__ngram_range=[(1,2)],\n",
        "    features__word__lowercase=[True],\n",
        "    features__word__stop_words=['english'],\n",
        "    features__word__strip_accents=['unicode'],\n",
        "    \n",
        "    features__char__max_features=[25000],\n",
        "    features__char__ngram_range=[(2,3)],\n",
        "    features__char__lowercase=[True],\n",
        "    features__char__strip_accents=['unicode'],\n",
        "    clf__loss=['modified_huber'],\n",
        "    clf__alpha=[.0001],\n",
        "    clf__learning_rate=['optimal'],\n",
        "    clf__eta0=[.001]\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "id": "sSuI81oYsskh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup classifier\n",
        "clf = SGDClassifier(verbose = 51) #Verbosity over 50 prints the entire log as it is fitted\n",
        "wVector = TfidfVectorizer(analyzer='word')\n",
        "cVector = TfidfVectorizer(analyzer='char')\n",
        "fUnion = FeatureUnion([(\"word\", wVector), (\"char\", cVector)])\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('features', fUnion),\n",
        "    ('clf', clf)\n",
        "])\n",
        "\n",
        "grid_search = GridSearchCV(pipe, param_grid=parameterGrid, n_jobs=6, pre_dispatch=4,\n",
        "                            verbose=51,cv=3, scoring='f1')"
      ],
      "metadata": {
        "id": "gNh2hsJNs2dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "grid_search.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "xVW3km0Xs9h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "y_valid_pred = grid_search.best_estimator_.predict(X_test)\n",
        "met = classification_report(y_test, y_valid_pred)\n",
        "print(met)"
      ],
      "metadata": {
        "id": "6qCDZ3YQtEip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix: Y-axis is what was predicted by the model, X-axis is what it should be\n",
        "conf_mat = confusion_matrix(y_test, y_valid_pred)\n",
        "print(conf_mat)"
      ],
      "metadata": {
        "id": "TVSb3ncatIwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lists best parameters from the grid search, borrowed from lecture code:\n",
        "print(\"Best parameters set:\")\n",
        "best_parameters = grid_search.best_estimator_.get_params()\n",
        "for param_name in sorted(parameterGrid.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
        "sys.stdout.flush()"
      ],
      "metadata": {
        "id": "NA2kTQmFtOHA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}